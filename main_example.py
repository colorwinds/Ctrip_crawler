"""
æºç¨‹æ•°æ®çˆ¬è™«ä¸»ç¨‹åºç¤ºä¾‹ / Ctrip Data Spider Main Example

æœ¬æ–‡ä»¶åŒ…å«å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹å’Œæ–‡æ¡£è¯´æ˜ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å„ä¸ªæ¨¡å—è¿›è¡Œæ•°æ®çˆ¬å–ã€‚
This file contains complete usage examples and documentation demonstrating how to use each module for data scraping.

ğŸš€ å¿«é€Ÿå¼€å§‹ / Quick Start
-------------------------

1. ç¯å¢ƒå‡†å¤‡ / Prerequisites:
   - Python 3.7+
   - å®‰è£…ä¾èµ– / Install dependencies:
     pip install requests beautifulsoup4

2. è¿è¡Œç¤ºä¾‹ / Run examples:
   python main_example.py

ğŸ“ è¾“å…¥å‚æ•°è¯´æ˜ / Input Parameters
----------------------------------

ç¤ºä¾‹1: æœç´¢æ™¯ç‚¹ID / Example 1: Search Attraction ID
- keyword: æ™¯ç‚¹å…³é”®è¯ï¼ˆå­—ç¬¦ä¸²ï¼‰/ Attraction keyword (string)
  - ç¤ºä¾‹ / Examples: "é»„é¹¤æ¥¼", "æ•…å®«", "å¤©å®‰é—¨"

ç¤ºä¾‹2: è·å–æ™¯ç‚¹åˆ—è¡¨ / Example 2: Get Attraction List
- district_id: åœ°åŒºIDï¼ˆæ•´æ•°ï¼‰/ District ID (integer)
  - å¸¸è§åœ°åŒºID / Common District IDs:
    9 = åŒ—äº¬ / Beijing
    2 = ä¸Šæµ· / Shanghai
    7 = å¹¿å· / Guangzhou
    26 = æ·±åœ³ / Shenzhen
    14 = æ­å· / Hangzhou
    104 = æˆéƒ½ / Chengdu
    6 = å—äº¬ / Nanjing
- page: é¡µç ï¼ˆæ•´æ•°ï¼Œé»˜è®¤1ï¼‰/ Page number (integer, default 1)
- count: æ¯é¡µæ•°é‡ï¼ˆæ•´æ•°ï¼Œé»˜è®¤20ï¼‰/ Count per page (integer, default 20)

ç¤ºä¾‹3: è·å–æ™¯ç‚¹è¯¦æƒ… / Example 3: Get Attraction Details
- poi_id: æ™¯ç‚¹POI IDï¼ˆæ•´æ•°ï¼‰/ Attraction POI ID (integer)
  - å¯ä»¥ä»æ™¯ç‚¹åˆ—è¡¨ä¸­è·å– / Can be obtained from attraction list

ç¤ºä¾‹4: çˆ¬å–è¯„è®º / Example 4: Scrape Comments
- poi_id: æ™¯ç‚¹POI IDï¼ˆå­—ç¬¦ä¸²ï¼‰/ Attraction POI ID (string)
- poi_name: æ™¯ç‚¹åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰/ Attraction name (string)
- max_pages: æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆæ•´æ•°ï¼Œé»˜è®¤100ï¼‰/ Max pages to scrape (integer, default 100)

ç¤ºä¾‹5: æ‰¹é‡çˆ¬å– / Example 5: Batch Scraping
- poi_list: æ™¯ç‚¹åˆ—è¡¨ï¼ˆåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [poi_id, poi_name]ï¼‰/ Attraction list (list, each element is [poi_id, poi_name])
- max_pages: æ¯ä¸ªæ™¯ç‚¹æœ€å¤§çˆ¬å–é¡µæ•° / Max pages per attraction

ğŸ“Š è¾“å‡ºæ–‡ä»¶è¯´æ˜ / Output Files
-------------------------------

1. æ™¯ç‚¹åˆ—è¡¨ (JSONæ ¼å¼) / Attraction List (JSON):
   - æ–‡ä»¶å / File: attractions_list.json
   - å­—æ®µ / Fields: name, id, poi_id, rating, review_count, price, address, etc.

2. è¯„è®ºæ•°æ® (CSVæ ¼å¼) / Comments (CSV):
   - æ–‡ä»¶å / File: {poi_id}_{æ™¯ç‚¹åç§°}.csv
   - ä½ç½® / Location: ./Datasets/
   - å­—æ®µ / Fields: è¯„è®ºID, ç”¨æˆ·æ˜µç§°, æ€»ä½“è¯„åˆ†, è¯„è®ºå†…å®¹, å‘å¸ƒæ—¶é—´, æœ‰ç”¨æ•°, å›å¤æ•°, 
     å‡ºè¡Œç±»å‹, ç”¨æˆ·æ‰€åœ¨åœ°, æ¸¸ç©æ—¶é•¿, å›¾ç‰‡æ•°é‡, å›¾ç‰‡é“¾æ¥åˆ—è¡¨, æ™¯è‰²è¯„åˆ†, è¶£å‘³è¯„åˆ†, 
     æ€§ä»·æ¯”è¯„åˆ†, æ¨èé¡¹ç›®

3. æ—¥å¿—æ–‡ä»¶ / Log Files:
   - ä½ç½® / Location: ./logs/
   - æ ¼å¼ / Format: {æ¨¡å—å}_{æ—¥æœŸ}.log

âš™ï¸ é…ç½®å‚æ•° / Configuration
---------------------------

- delay_range: è¯·æ±‚å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰/ Request delay range (seconds)
  æ¨èå€¼ / Recommended: (1, 3)
- use_user_agent_rotation: User-Agentè½®æ¢ / User-Agent rotation
  æ¨èå€¼ / Recommended: True
- use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç† / Use proxy (optional)
  éœ€è¦é…ç½®proxiesåˆ—è¡¨ / Requires proxies list

âš ï¸ æ³¨æ„äº‹é¡¹ / Important Notes
-----------------------------

1. éµå®ˆåè®® / Compliance: è¯·éµå®ˆç½‘ç«™çš„robots.txtå’Œä½¿ç”¨åè®®
2. åˆç†é¢‘ç‡ / Rate Limiting: å»ºè®®è®¾ç½®åˆç†çš„å»¶è¿ŸèŒƒå›´ï¼ˆ1-3ç§’ï¼‰
3. æ•°æ®ä½¿ç”¨ / Data Usage: çˆ¬å–çš„æ•°æ®ä»…ä¾›å­¦ä¹ ç ”ç©¶ä½¿ç”¨
4. é”™è¯¯å¤„ç† / Error Handling: å»ºè®®ç›‘æ§æ—¥å¿—æ–‡ä»¶
5. ä»£ç†ä½¿ç”¨ / Proxy Usage: å¦‚éœ€ä½¿ç”¨ä»£ç†ï¼Œè¯·ç¡®ä¿ä»£ç†å¯ç”¨æ€§

ğŸ› å¸¸è§é—®é¢˜ / Troubleshooting
------------------------------

Q1: æ— æ³•è·å–æ•°æ®ï¼Ÿ/ Cannot retrieve data?
- æ£€æŸ¥ç½‘ç»œè¿æ¥ / Check network connection
- æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£é”™è¯¯è¯¦æƒ… / Review log files for error details
- å°è¯•å¢åŠ å»¶è¿Ÿæ—¶é—´ / Try increasing delay time
- æ£€æŸ¥æ˜¯å¦è¢«é™åˆ¶è®¿é—® / Check if access is restricted

Q2: è¯„è®ºçˆ¬å–å¤±è´¥ï¼Ÿ/ Comment scraping failed?
- ç¡®è®¤POI IDæ˜¯å¦æ­£ç¡® / Verify POI ID is correct
- æ£€æŸ¥æ™¯ç‚¹æ˜¯å¦æœ‰è¯„è®º / Check if the attraction has comments
- æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£å…·ä½“é”™è¯¯ / Review log files for specific errors

Q3: å¦‚ä½•è·å–æ›´å¤šåœ°åŒºIDï¼Ÿ/ How to get more district IDs?
- ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·æŸ¥çœ‹æºç¨‹ç½‘ç«™çš„ç½‘ç»œè¯·æ±‚ / Use browser developer tools
- æˆ–ä½¿ç”¨ sight_list.py å°è¯•ä¸åŒçš„district_id / Or try different district_id values
"""
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path / Add project path to sys.path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from Ctrip_Spider.log import CtripSpiderLogger
from Ctrip_Spider.sight_id import SightId
from Ctrip_Spider.sight_list import CtripAttractionScraper
from Ctrip_Spider.sight_detail import AttractionDetailFetcher
from Ctrip_Spider.sight_comments import CtripCommentSpider


# å¸¸è§åœ°åŒºIDå‚è€ƒ / Common District IDs Reference
COMMON_DISTRICT_IDS = {
    9: "åŒ—äº¬ / Beijing",
    2: "ä¸Šæµ· / Shanghai",
    7: "å¹¿å· / Guangzhou",
    26: "æ·±åœ³ / Shenzhen",
    14: "æ­å· / Hangzhou",
    104: "æˆéƒ½ / Chengdu",
    6: "å—äº¬ / Nanjing",
}


def example_1_search_sight_id():
    """
    ç¤ºä¾‹1: æ ¹æ®å…³é”®è¯æœç´¢æ™¯ç‚¹ID / Example 1: Search Attraction ID by Keyword
    
    è¾“å…¥å‚æ•° / Input Parameters:
        keyword: æ™¯ç‚¹å…³é”®è¯ï¼ˆå­—ç¬¦ä¸²ï¼‰/ Attraction keyword (string)
        ç¤ºä¾‹ / Examples: "é»„é¹¤æ¥¼", "æ•…å®«", "å¤©å®‰é—¨"
    
    è¿”å› / Returns:
        dict: {å…³é”®è¯: æ™¯ç‚¹ID} / {keyword: sight_id}
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹1: æœç´¢æ™¯ç‚¹ID / Example 1: Search Attraction ID")
    print("="*60)
    
    logger = CtripSpiderLogger("SearchSightId", "logs")
    
    # åˆ›å»ºæ™¯ç‚¹IDæœç´¢å™¨ / Create attraction ID searcher
    sight_id_searcher = SightId(
        delay_range=(1, 2),  # è¯·æ±‚å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰/ Request delay range (seconds)
        use_user_agent_rotation=True,  # å¯ç”¨User-Agentè½®æ¢ / Enable User-Agent rotation
        logger=logger
    )
    
    # æœç´¢å…³é”®è¯åˆ—è¡¨ / Search keyword list
    keywords = ["é»„é¹¤æ¥¼", "æ•…å®«", "å¤©å®‰é—¨"]
    
    print("\nå¼€å§‹æœç´¢æ™¯ç‚¹ID... / Starting to search attraction IDs...")
    results = {}
    
    for keyword in keywords:
        print(f"\næœç´¢å…³é”®è¯ / Searching keyword: {keyword}")
        sight_id = sight_id_searcher.search_sight_id(keyword)
        
        if sight_id:
            results[keyword] = sight_id
            print(f"  âœ“ æ‰¾åˆ°æ™¯ç‚¹ID / Found attraction ID: {sight_id}")
        else:
            print(f"  âœ— æœªæ‰¾åˆ°æ™¯ç‚¹ / Attraction not found")
    
    return results


def example_2_get_attraction_list():
    """
    ç¤ºä¾‹2: è·å–æ™¯ç‚¹åˆ—è¡¨ / Example 2: Get Attraction List
    
    è¾“å…¥å‚æ•° / Input Parameters:
        district_id: åœ°åŒºIDï¼ˆæ•´æ•°ï¼‰/ District ID (integer)
            - 9 = åŒ—äº¬ / Beijing
            - 2 = ä¸Šæµ· / Shanghai
            - 7 = å¹¿å· / Guangzhou
            - æ›´å¤šåœ°åŒºIDè§ COMMON_DISTRICT_IDS / See COMMON_DISTRICT_IDS for more
        pages: é¡µæ•°ï¼ˆæ•´æ•°ï¼‰/ Number of pages (integer)
        count_per_page: æ¯é¡µæ•°é‡ï¼ˆæ•´æ•°ï¼‰/ Count per page (integer)
    
    è¿”å› / Returns:
        list: æ™¯ç‚¹åˆ—è¡¨ / List of attractions
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹2: è·å–æ™¯ç‚¹åˆ—è¡¨ / Example 2: Get Attraction List")
    print("="*60)
    
    logger = CtripSpiderLogger("GetAttractionList", "logs")
    
    # åˆ›å»ºæ™¯ç‚¹åˆ—è¡¨çˆ¬å–å™¨ / Create attraction list scraper
    scraper = CtripAttractionScraper(
        timeout=10,
        delay_range=(1, 2),  # è¯·æ±‚å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰/ Request delay range (seconds)
        use_user_agent_rotation=True,  # å¯ç”¨User-Agentè½®æ¢ / Enable User-Agent rotation
        logger=logger
    )
    
    # åœ°åŒºIDè¯´æ˜ / District ID notes:
    # 9 = åŒ—äº¬ / Beijing
    # 2 = ä¸Šæµ· / Shanghai
    # 7 = å¹¿å· / Guangzhou
    # å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ / Can be modified as needed
    district_id = 9  # åŒ—äº¬ / Beijing
    pages = 2  # è·å–2é¡µæ•°æ® / Get 2 pages of data
    count_per_page = 5  # æ¯é¡µ5ä¸ªæ™¯ç‚¹ / 5 attractions per page
    
    print(f"\nè·å–åœ°åŒºID {district_id} çš„æ™¯ç‚¹åˆ—è¡¨... / Getting attraction list for district ID {district_id}...")
    print(f"é¡µæ•° / Pages: {pages}, æ¯é¡µæ•°é‡ / Count per page: {count_per_page}")
    
    attractions = scraper.get_attractions_with_pagination(
        district_id=district_id,
        pages=pages,
        count_per_page=count_per_page
    )
    
    if attractions:
        print(f"\nâœ“ æˆåŠŸè·å– {len(attractions)} ä¸ªæ™¯ç‚¹ / Successfully retrieved {len(attractions)} attractions")
        print("\næ™¯ç‚¹åˆ—è¡¨ / Attraction List:")
        for i, attr in enumerate(attractions[:10], 1):  # æ˜¾ç¤ºå‰10ä¸ª / Show first 10
            print(f"\n{i}. {attr['name']}")
            print(f"   ID: {attr['id']}")
            print(f"   è¯„åˆ† / Rating: {attr['rating']} (åŸºäº{attr['review_count']}æ¡è¯„è®º / based on {attr['review_count']} reviews)")
            print(f"   ä»·æ ¼ / Price: {attr.get('price', 0)}å…ƒ")
            print(f"   åœ°å€ / Address: {attr.get('address', 'æœªçŸ¥ / Unknown')}")
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶ / Save to JSON file
        output_file = './attractions_list.json'
        scraper.save_to_json(attractions, output_file)
        print(f"\nâœ“ æ•°æ®å·²ä¿å­˜åˆ° / Data saved to: {output_file}")
        
        return attractions
    else:
        print("\nâœ— æœªè·å–åˆ°æ™¯ç‚¹æ•°æ® / No attraction data retrieved")
        return []


def example_3_get_attraction_detail():
    """
    ç¤ºä¾‹3: è·å–æ™¯ç‚¹è¯¦ç»†ä¿¡æ¯ / Example 3: Get Attraction Details
    
    è¾“å…¥å‚æ•° / Input Parameters:
        poi_id: æ™¯ç‚¹POI IDï¼ˆæ•´æ•°ï¼‰/ Attraction POI ID (integer)
        - å¯ä»¥ä»æ™¯ç‚¹åˆ—è¡¨ä¸­è·å– / Can be obtained from attraction list
    
    è¿”å› / Returns:
        list: æ™¯ç‚¹è¯¦æƒ…åˆ—è¡¨ / List of attraction details
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹3: è·å–æ™¯ç‚¹è¯¦ç»†ä¿¡æ¯ / Example 3: Get Attraction Details")
    print("="*60)
    
    logger = CtripSpiderLogger("GetAttractionDetail", "logs")
    
    # åˆ›å»ºæ™¯ç‚¹è¯¦æƒ…è·å–å™¨ / Create attraction detail fetcher
    detail_fetcher = AttractionDetailFetcher(
        delay_range=(1, 2),  # è¯·æ±‚å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰/ Request delay range (seconds)
        use_user_agent_rotation=True,  # å¯ç”¨User-Agentè½®æ¢ / Enable User-Agent rotation
        logger=logger
    )
    
    # æ™¯ç‚¹POI IDåˆ—è¡¨ï¼ˆå¯ä»¥ä»æ™¯ç‚¹åˆ—è¡¨ä¸­è·å–ï¼‰/ Attraction POI ID list (can be obtained from attraction list)
    poi_ids = [87211, 76865]  # ç¤ºä¾‹POI ID / Example POI IDs
    
    print("\nå¼€å§‹è·å–æ™¯ç‚¹è¯¦æƒ…... / Starting to fetch attraction details...")
    details = []
    
    for poi_id in poi_ids:
        print(f"\nè·å–POI ID / Fetching POI ID: {poi_id}")
        detail = detail_fetcher.get_detail(poi_id)
        
        if detail.get('success'):
            details.append(detail)
            print(f"  âœ“ æˆåŠŸè·å–è¯¦æƒ… / Successfully fetched details")
            print(f"  æ™¯ç‚¹åç§° / Attraction Name: {detail.get('poi_name', 'æœªçŸ¥ / Unknown')}")
            print(f"  è‹±æ–‡å / English Name: {detail.get('english_name', 'æœªçŸ¥ / Unknown')}")
            print(f"  æ‰€åœ¨åœ°åŒº / District: {detail.get('district', 'æœªçŸ¥ / Unknown')}")
            print(f"  é—¨ç¥¨ä»·æ ¼ / Ticket Price: {detail.get('ticket_price', 'æœªçŸ¥ / Unknown')}")
            print(f"  è”ç³»ç”µè¯ / Telephone: {', '.join(detail.get('telephone', []))}")
        else:
            print(f"  âœ— è·å–å¤±è´¥ / Failed to fetch: {detail.get('error_message', 'æœªçŸ¥é”™è¯¯ / Unknown error')}")
    
    return details


def example_4_crawl_comments():
    """
    ç¤ºä¾‹4: çˆ¬å–æ™¯ç‚¹è¯„è®º / Example 4: Scrape Attraction Comments
    
    è¾“å…¥å‚æ•° / Input Parameters:
        poi_id: æ™¯ç‚¹POI IDï¼ˆå­—ç¬¦ä¸²ï¼‰/ Attraction POI ID (string)
        poi_name: æ™¯ç‚¹åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰/ Attraction name (string)
        max_pages: æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆæ•´æ•°ï¼‰/ Maximum pages to scrape (integer)
    
    è¿”å› / Returns:
        dict: çˆ¬å–ç»“æœ / Scraping results
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹4: çˆ¬å–æ™¯ç‚¹è¯„è®º / Example 4: Scrape Attraction Comments")
    print("="*60)
    
    logger = CtripSpiderLogger("CrawlComments", "logs")
    
    # åˆ›å»ºè¯„è®ºçˆ¬è™« / Create comment spider
    comment_spider = CtripCommentSpider(
        output_dir='./Datasets',  # è¾“å‡ºç›®å½• / Output directory
        delay_range=(1, 2),  # æ¯æ¬¡è¯·æ±‚å»¶è¿Ÿ1-2ç§’ / Delay 1-2 seconds per request
        use_user_agent_rotation=True,  # å¯ç”¨User-Agentè½®æ¢ / Enable User-Agent rotation
        logger=logger
    )
    
    # æ™¯ç‚¹åˆ—è¡¨ï¼šæ ¼å¼ä¸º [poi_id, poi_name] / Attraction list: format [poi_id, poi_name]
    pois = [
        ['76865', 'æ˜Ÿæµ·å¹¿åœº'],
        ['75628', 'æ£’æ£°å²›'],
    ]
    
    print("\nå¼€å§‹çˆ¬å–è¯„è®º... / Starting to scrape comments...")
    print(f"æ™¯ç‚¹æ•°é‡ / Number of attractions: {len(pois)}")
    print(f"è¾“å‡ºç›®å½• / Output directory: {comment_spider.output_dir}")
    
    # æ‰¹é‡çˆ¬å–è¯„è®º / Batch scrape comments
    results = comment_spider.crawl_multiple_pois(
        poi_list=pois,
        max_pages=3  # æ¯ä¸ªæ™¯ç‚¹çˆ¬å–3é¡µï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰/ Scrape 3 pages per attraction (adjustable)
    )
    
    # æ˜¾ç¤ºç»“æœ / Display results
    print("\nçˆ¬å–ç»“æœæ±‡æ€» / Scraping Results Summary:")
    for poi, success in results.items():
        status = "âœ“ æˆåŠŸ / Success" if success else "âœ— å¤±è´¥ / Failed"
        print(f"  {poi}: {status}")
    
    return results


def example_5_complete_workflow():
    """
    ç¤ºä¾‹5: å®Œæ•´å·¥ä½œæµç¨‹ - ä»æœç´¢åˆ°çˆ¬å–è¯„è®º / Example 5: Complete Workflow - From Search to Comment Scraping
    
    æ¼”ç¤ºå®Œæ•´çš„æ•°æ®çˆ¬å–æµç¨‹ï¼š
    1. æœç´¢æ™¯ç‚¹ID
    2. è·å–æ™¯ç‚¹åˆ—è¡¨
    3. è·å–æ™¯ç‚¹è¯¦æƒ…
    4. çˆ¬å–è¯„è®º
    
    Demonstrates complete data scraping workflow:
    1. Search attraction ID
    2. Get attraction list
    3. Get attraction details
    4. Scrape comments
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹5: å®Œæ•´å·¥ä½œæµç¨‹ / Example 5: Complete Workflow")
    print("="*60)
    
    logger = CtripSpiderLogger("CompleteWorkflow", "logs")
    
    # æ­¥éª¤1: æœç´¢æ™¯ç‚¹ID / Step 1: Search Attraction ID
    print("\n[æ­¥éª¤1 / Step 1] æœç´¢æ™¯ç‚¹ID / Searching Attraction ID...")
    keyword = "é»„é¹¤æ¥¼"
    sight_id_searcher = SightId(
        delay_range=(1, 2),
        use_user_agent_rotation=True,
        logger=logger
    )
    sight_id = sight_id_searcher.search_sight_id(keyword)
    
    if not sight_id:
        print(f"æœªæ‰¾åˆ°å…³é”®è¯ '{keyword}' å¯¹åº”çš„æ™¯ç‚¹IDï¼Œç»ˆæ­¢æµç¨‹ / No attraction ID found for keyword '{keyword}', terminating workflow")
        return
    
    print(f"âœ“ æ‰¾åˆ°æ™¯ç‚¹ID / Found attraction ID: {sight_id}")
    
    # æ­¥éª¤2: è·å–æ™¯ç‚¹åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦POI IDï¼‰/ Step 2: Get Attraction List (optional, if POI ID needed)
    print("\n[æ­¥éª¤2 / Step 2] è·å–æ™¯ç‚¹åˆ—è¡¨ / Getting Attraction List...")
    scraper = CtripAttractionScraper(
        timeout=10,
        delay_range=(1, 2),
        use_user_agent_rotation=True,
        logger=logger
    )
    attractions = scraper.get_attractions_list(district_id=9, page=1, count=10)
    
    if attractions:
        print(f"âœ“ è·å–åˆ° {len(attractions)} ä¸ªæ™¯ç‚¹ / Retrieved {len(attractions)} attractions")
        # æ‰¾åˆ°åŒ¹é…çš„æ™¯ç‚¹ / Find matching attraction
        target_attraction = None
        for attr in attractions:
            if attr.get('id') == sight_id:
                target_attraction = attr
                break
        
        if target_attraction:
            poi_id = target_attraction.get('poi_id')
            poi_name = target_attraction.get('name')
            print(f"âœ“ æ‰¾åˆ°ç›®æ ‡æ™¯ç‚¹ / Found target attraction: {poi_name} (POI ID: {poi_id})")
            
            # æ­¥éª¤3: è·å–æ™¯ç‚¹è¯¦æƒ… / Step 3: Get Attraction Details
            print("\n[æ­¥éª¤3 / Step 3] è·å–æ™¯ç‚¹è¯¦æƒ… / Getting Attraction Details...")
            detail_fetcher = AttractionDetailFetcher(
                delay_range=(1, 2),
                use_user_agent_rotation=True,
                logger=logger
            )
            detail = detail_fetcher.get_detail(poi_id)
            
            if detail.get('success'):
                print(f"âœ“ æˆåŠŸè·å–è¯¦æƒ… / Successfully fetched details")
                print(f"  æè¿° / Description: {detail.get('description', '')[:100]}...")
            
            # æ­¥éª¤4: çˆ¬å–è¯„è®º / Step 4: Scrape Comments
            print("\n[æ­¥éª¤4 / Step 4] çˆ¬å–è¯„è®º / Scraping Comments...")
            comment_spider = CtripCommentSpider(
                output_dir='./Datasets',
                delay_range=(1, 2),
                use_user_agent_rotation=True,
                logger=logger
            )
            success = comment_spider.crawl_comments(
                poi_id=str(poi_id),
                poi_name=poi_name,
                max_pages=2  # çˆ¬å–2é¡µä½œä¸ºç¤ºä¾‹ / Scrape 2 pages as example
            )
            
            if success:
                print(f"âœ“ è¯„è®ºçˆ¬å–å®Œæˆï¼Œæ•°æ®ä¿å­˜åœ¨ / Comment scraping completed, data saved in: ./Datasets/")
            else:
                print("âœ— è¯„è®ºçˆ¬å–å¤±è´¥ / Comment scraping failed")
        else:
            print("æœªåœ¨åˆ—è¡¨ä¸­æ‰¾åˆ°åŒ¹é…çš„æ™¯ç‚¹ / Matching attraction not found in list")
    else:
        print("æœªè·å–åˆ°æ™¯ç‚¹åˆ—è¡¨ / No attraction list retrieved")


def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹ / Main Function - Run All Examples
    """
    print("="*60)
    print("æºç¨‹æ•°æ®çˆ¬è™« - ä½¿ç”¨ç¤ºä¾‹ / Ctrip Data Spider - Usage Examples")
    print("="*60)
    print("\næœ¬ç¨‹åºæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å„ä¸ªæ¨¡å—è¿›è¡Œæ•°æ®çˆ¬å–")
    print("This program demonstrates how to use each module for data scraping")
    print("æ³¨æ„: è¯·éµå®ˆç½‘ç«™ä½¿ç”¨åè®®ï¼Œåˆç†æ§åˆ¶çˆ¬å–é¢‘ç‡")
    print("Note: Please comply with website terms of service and control scraping frequency reasonably")
    
    # åˆ›å»ºè¾“å‡ºç›®å½• / Create output directories
    os.makedirs('./Datasets', exist_ok=True)
    os.makedirs('./logs', exist_ok=True)
    
    try:
        # è¿è¡Œå„ä¸ªç¤ºä¾‹ï¼ˆå¯ä»¥æ³¨é‡Šæ‰ä¸éœ€è¦çš„ç¤ºä¾‹ï¼‰/ Run examples (can comment out unwanted examples)
        
        # ç¤ºä¾‹1: æœç´¢æ™¯ç‚¹ID / Example 1: Search Attraction ID
        example_1_search_sight_id()
        
        # ç¤ºä¾‹2: è·å–æ™¯ç‚¹åˆ—è¡¨ / Example 2: Get Attraction List
        attractions = example_2_get_attraction_list()
        
        # ç¤ºä¾‹3: è·å–æ™¯ç‚¹è¯¦æƒ… / Example 3: Get Attraction Details
        example_3_get_attraction_detail()
        
        # ç¤ºä¾‹4: çˆ¬å–è¯„è®º / Example 4: Scrape Comments
        example_4_crawl_comments()
        
        # ç¤ºä¾‹5: å®Œæ•´å·¥ä½œæµç¨‹ï¼ˆå¯é€‰ï¼‰/ Example 5: Complete Workflow (optional)
        # example_5_complete_workflow()
        
        print("\n" + "="*60)
        print("æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼/ All examples completed!")
        print("="*60)
        print("\nè¾“å‡ºæ–‡ä»¶ / Output Files:")
        print("  - æ™¯ç‚¹åˆ—è¡¨ / Attraction List: ./attractions_list.json")
        print("  - è¯„è®ºæ•°æ® / Comments Data: ./Datasets/*.csv")
        print("  - æ—¥å¿—æ–‡ä»¶ / Log Files: ./logs/*.log")
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº / User interrupted program")
    except Exception as e:
        print(f"\n\nå‘ç”Ÿé”™è¯¯ / Error occurred: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
